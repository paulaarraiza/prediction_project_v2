{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e01a108-b015-4815-9bd8-aa384c980758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c3bb6-8947-47d2-8fc8-b2a9c12c56f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote(df, device):\n",
    "\n",
    "    X = np.array([np.stack(row) for row in df.drop(columns=['Target']).values])\n",
    "    y = df['Target'].values\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    n_samples, timesteps, n_features = X.shape\n",
    "    X_flat = X.reshape((n_samples, timesteps * n_features))\n",
    "\n",
    "    # Apply SMOTE\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_flat, y)\n",
    "    X_resampled = X_resampled.reshape((-1, timesteps, n_features))\n",
    "    \n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00f10f-3e72-4311-acaf-7c68492b56f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tensor(X_resampled, y_resampled, test_size, device):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size, random_state=42)\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    trainloader = DataLoader(train_dataset, batch_size, shuffle=False)\n",
    "    testloader = DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "    \n",
    "    return train_dataset, test_dataset, trainloader, testloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c8b4a7-de9a-4e30-a2b3-090c3c788fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU3DClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(GRU3DClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)  # Only use if suitable for your classification type\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        # Forward propagate through GRU\n",
    "        out, _ = self.gru(x, h0)\n",
    "        # Take output from the last time step\n",
    "        out = self.fc(out[:, -1, :])  # Shape: (batch_size, output_size)\n",
    "        return self.softmax(out)  # Consider using this only if it's suitable for your classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67eeb77-8042-49ad-aa7f-474c73c017ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRU3DClassifier(input_size, hidden_size, output_size, num_layers, dropout)\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462f60c-0494-4cc0-959f-b30fcf30caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = X.shape[2]\n",
    "hidden_size = 64  \n",
    "output_size = 2  \n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "learning_rate = 0.1\n",
    "batch_size = 32\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1844319-e032-426d-bc04-cafb4a4707fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, num_epochs, trainloader, criterion, device):\n",
    "\n",
    "    losses = []\n",
    "    predictions_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() \n",
    "        running_loss = 0.0\n",
    "        epoch_predictions = []\n",
    "        epoch_realized = []\n",
    "        for X_batch, y_batch in trainloader:\n",
    "            X_batch = X_batch.float().to(device)  # Ensure correct data type and device\n",
    "            y_batch = y_batch.long().to(device)\n",
    "\n",
    "            # X_batch = X_batch.permute(0, 2, 1) \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred_y = model(X_batch)\n",
    "            loss = criterion(pred_y, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Detach predictions and convert to CPU for analysis (if using GPU)\n",
    "            epoch_predictions.append(pred_y.detach().cpu().numpy())\n",
    "            epoch_realized.append(y_batch.detach().cpu().numpy())\n",
    "\n",
    "        # Calculate the average loss for the epoch\n",
    "        average_loss = running_loss / len(trainloader)\n",
    "\n",
    "        # Store the average loss for this epoch\n",
    "        losses.append(average_loss)\n",
    "        epoch_predictions = np.concatenate(epoch_predictions, axis=0)\n",
    "        epoch_realized = np.concatenate(epoch_realized, axis=0)\n",
    "        predictions_list.append(epoch_predictions)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(trainloader)}\")\n",
    "\n",
    "    # After training, plot the loss\n",
    "    plt.plot(range(1, num_epochs+1), losses, marker='o', label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    predicted_y = np.argmax(predictions_list[-1], axis=1)  # Shape: (1000,)\n",
    "    proportion_pred_over_0_5 = np.mean(predicted_y)\n",
    "    proportion_realised_ones = np.mean(np.array(epoch_realized) == 1)  # Ensure epoch_realized is an array\n",
    "\n",
    "    print(f\"Proportion of Predicted 1's: {proportion_pred_over_0_5:.2f}\\n\"\n",
    "        f\"Proportion of Realized 1's: {proportion_realised_ones:.2f}\")\n",
    "    \n",
    "    conf_matrix = confusion_matrix(epoch_realized, predicted_y)\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "                yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix (Last Epoch)')\n",
    "    plt.show()\n",
    "\n",
    "    TN, FP, FN, TP = conf_matrix.ravel()  # Unravel the confusion matrix\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "    # Print accuracy\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9d429c-e7ea-463a-8902-88c1a39e6b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, testloader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for X_batch, y_batch in dataloader:\n",
    "    \n",
    "            X_batch = X_batch.float().to(device)\n",
    "            y_batch = y_batch.long().to(device)\n",
    "            \n",
    "            \n",
    "            pred_y = model(X_batch)\n",
    "            \n",
    "            loss = criterion(pred_y, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_predictions.append(pred_y.detach().cpu().numpy())\n",
    "            all_targets.append(y_batch.detach().cpu().numpy())\n",
    "    \n",
    "    # Calculate average loss\n",
    "    average_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    # Concatenate all predictions and targets\n",
    "    all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)\n",
    "    \n",
    "    predicted_classes = np.argmax(all_predictions, axis=1)  # Assuming softmax is used and you want class labels\n",
    "    accuracy = accuracy_score(all_targets, predicted_classes)\n",
    "    \n",
    "    print(f\"Test Loss: {average_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return accuracy\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
